### Q 1 : What is gradient descent ? ###

Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak 
parameters iteratively in order to minimize the cost function. The rest of the answer is [here](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=4s)

### Q 2 : What is feature engineering ? ###

Feature engineering is the process of transforming raw data into features that are suitable for machine learning models. In other words, it is the process of selecting, extracting, 
and transforming the most relevant features from the available data to build more accurate and efficient machine learning models.

The success of machine learning models heavily depends on the quality of the features used to train them. Feature engineering involves a set of techniques that enable us to create 
new features by combining or transforming the existing ones. These techniques help to highlight the most important patterns and relationships in the data, which in turn helps the 
machine learning model to learn from the data more effectively.

### Q 3 : What is feature mapping ? ###

Feature mapping is a technique used in data analysis and machine learning to transform input data from a lower-dimensional space to a higher-dimensional space, where it can be 
more easily analyzed or classified. The rest of the answer is [here](https://www.geeksforgeeks.org/feature-mapping/) 

### Q 4 : What is a neural network ? ###

Neural Network is a sophisticated architecture consist of a stack of layers and neurons in each layer. Neural Network is the mathematical functions which transfer input variables 
to the target variable and learn the patterns.

A Neural Network is basically a dense interconnection of layers, which are further made up of basic units called perceptrons. A perceptron consists of input terminals, the 
processing unit and the output terminals. The input terminals of a perceptron are connected to the output terminals of the preceding perceptrons. The rest of the answer is [here](https://www.youtube.com/watch?v=aircAruvnKk)

### Q 5 : Why do we need to perform normalization or feature scaling ? What are the different types of normalization ? Also what is z-score normalization ? ###

Normalization or feature scaling is a way to make sure that features with very diverse ranges will proportionally impact the network performance. Without normalization, some 
features or variables might be ignored. For example, imagine that we want to predict the price of a car using two features such as the driven distance and the car’s age. 
The first feature’s range is in thousands whereas the second one is in the range of tens of years. Using the raw data for predicting the price of the car, the distance feature 
would outweigh the age feature significantly. Therefore, we should normalize these two features to get a more accurate prediction.

The different types of normalization are as follows : The rest of the answer is [here](https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf)

Z-score normalization refers to the process of normalizing every value in a dataset such that the mean of all of the values is 0 and the standard deviation is 1. We use the 
following formula to perform a z-score normalization on every value in a dataset : `New value = (x – μ) / σ` where : `x: Original value, μ: Mean of data and σ: Standard deviation of data`

### Q 6 : What is the difference between normalization or feature scaling and standarization ? ###

Normalisation is suitable to use when the data does not follow Gaussian Distribution principles. It can be used in algorithms that do not assume data distribution, such as 
K-Nearest Neighbors and Neural Networks. The rest of the answer is [here](https://www.codingninjas.com/studio/library/normalisation-vs-standardisation). More visual 
understanding refer to this [link](https://www.youtube.com/watch?v=sxEqtjLC0aM)

### Q 7 : Explain the "Bias-Variance Tradeoff" in Machine Learning. ###

In detail : Imagine a scenario in which a model works perfectly well with the data it was trained on, but provides incorrect predictions when it meets new, unfamiliar data. The rest of the answer is [here](https://serokell.io/blog/bias-variance-tradeoff)

In brief : In order to evaluate the performance of the model, we need to look at the amount of error it’s making. The rest of the answer is [here](https://towardsdatascience.com/bias-variance-trade-off-overfitting-regularization-in-machine-learning-d79c6d8f20b4)

### Q 8 : Explain overfitting and underfitting. ###

Overfitting happens when we train a machine learning model too much tuned to the training set. As a result, the model learns the training data too well, but it can’t generate good predictions for unseen data. An overfitted model produces low accuracy results for data points unseen in training, hence, leads to non-optimal decisions.

Underfitting occurs when the machine learning model is not well-tuned to the training set. The resulting model is not capturing the relationship between input and output well enough. Therefore, it doesn’t produce accurate predictions, even for the training dataset. Resultingly, an underfitted model generates poor results that lead to high-error decisions, like an overfitted model.

![image](https://github.com/roy-sub/Data-Scientist-Interview-Course/blob/main/Figures/overfitting%20vs%20underfitting.png)

Reducing the error from overfitting or underfitting is referred to as the bias-variance tradeoff. We aim to find a good fitting model in between. More further details refer to this [link](https://www.baeldung.com/cs/ml-underfitting-overfitting)

### Q 9 : Explain Confusion Matrix, Accuracy, Precision, Recall and F1 Score. ###

Refer to the following links [video tutorial](https://www.youtube.com/watch?v=Kdsp6soqA7o), [blog i](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), [blog ii](https://proclusacademy.com/blog/explainer/confusion-matrix-accuracy-classification-models/) and [blog iii](https://proclusacademy.com/blog/explainer/precision-recall-f1-score-classification-models/)

### Q 10 : Explain the KNN Algorithm. ###

KNN is a simple algorithm, based on the local minimum of the target function which is used to learn an unknown function of desired precision and accuracy. The rest of the answer is [here](https://neptune.ai/blog/knn-algorithm-explanation-opportunities-limitations). And for visual understanding refer this [link](https://www.youtube.com/watch?v=HVXime0nQeI)

