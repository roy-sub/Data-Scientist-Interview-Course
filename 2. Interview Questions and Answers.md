### Q 1 : What is gradient descent ? ###

Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak 
parameters iteratively in order to minimize the cost function. The rest of the answer is [here](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=4s)

### Q 2 : What is feature engineering ? ###

Feature engineering is the process of transforming raw data into features that are suitable for machine learning models. In other words, it is the process of selecting, extracting, 
and transforming the most relevant features from the available data to build more accurate and efficient machine learning models.

The success of machine learning models heavily depends on the quality of the features used to train them. Feature engineering involves a set of techniques that enable us to create 
new features by combining or transforming the existing ones. These techniques help to highlight the most important patterns and relationships in the data, which in turn helps the 
machine learning model to learn from the data more effectively.

### Q 3 : What is feature mapping ? ###

Feature mapping is a technique used in data analysis and machine learning to transform input data from a lower-dimensional space to a higher-dimensional space, where it can be 
more easily analyzed or classified. The rest of the answer is [here](https://www.geeksforgeeks.org/feature-mapping/) 

### Q 4 : What is a neural network ? ###

Neural Network is a sophisticated architecture consist of a stack of layers and neurons in each layer. Neural Network is the mathematical functions which transfer input variables 
to the target variable and learn the patterns.

A Neural Network is basically a dense interconnection of layers, which are further made up of basic units called perceptrons. A perceptron consists of input terminals, the 
processing unit and the output terminals. The input terminals of a perceptron are connected to the output terminals of the preceding perceptrons. The rest of the answer is [here](https://www.youtube.com/watch?v=aircAruvnKk)

### Q 5 : Why do we need to perform normalization or feature scaling ? What are the different types of normalization ? Also what is z-score normalization ? ###

Normalization or feature scaling is a way to make sure that features with very diverse ranges will proportionally impact the network performance. Without normalization, some 
features or variables might be ignored. For example, imagine that we want to predict the price of a car using two features such as the driven distance and the car’s age. 
The first feature’s range is in thousands whereas the second one is in the range of tens of years. Using the raw data for predicting the price of the car, the distance feature 
would outweigh the age feature significantly. Therefore, we should normalize these two features to get a more accurate prediction.

The different types of normalization are as follows : The rest of the answer is [here](https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf)

Z-score normalization refers to the process of normalizing every value in a dataset such that the mean of all of the values is 0 and the standard deviation is 1. We use the 
following formula to perform a z-score normalization on every value in a dataset : `New value = (x – μ) / σ` where : `x: Original value, μ: Mean of data and σ: Standard deviation of data`

### Q 6 : What is the difference between normalization or feature scaling and standarization ? ###

Normalisation is suitable to use when the data does not follow Gaussian Distribution principles. It can be used in algorithms that do not assume data distribution, such as 
K-Nearest Neighbors and Neural Networks. The rest of the answer is [here](https://www.codingninjas.com/studio/library/normalisation-vs-standardisation). More visual 
understanding refer to this [link](https://www.youtube.com/watch?v=sxEqtjLC0aM)

### Q 7 : Explain the "Bias-Variance Tradeoff" in Machine Learning. ###

In detail : Imagine a scenario in which a model works perfectly well with the data it was trained on, but provides incorrect predictions when it meets new, unfamiliar data. The rest of the answer is [here](https://serokell.io/blog/bias-variance-tradeoff)

In brief : In order to evaluate the performance of the model, we need to look at the amount of error it’s making. The rest of the answer is [here](https://towardsdatascience.com/bias-variance-trade-off-overfitting-regularization-in-machine-learning-d79c6d8f20b4)

### Q 8 : Explain overfitting and underfitting. ###

Overfitting happens when we train a machine learning model too much tuned to the training set. As a result, the model learns the training data too well, but it can’t generate good predictions for unseen data. An overfitted model produces low accuracy results for data points unseen in training, hence, leads to non-optimal decisions.

Underfitting occurs when the machine learning model is not well-tuned to the training set. The resulting model is not capturing the relationship between input and output well enough. Therefore, it doesn’t produce accurate predictions, even for the training dataset. Resultingly, an underfitted model generates poor results that lead to high-error decisions, like an overfitted model.

![image](https://github.com/roy-sub/Data-Scientist-Interview-Course/blob/main/Figures/overfitting%20vs%20underfitting.png)

Reducing the error from overfitting or underfitting is referred to as the bias-variance tradeoff. We aim to find a good fitting model in between. More further details refer to this [link](https://www.baeldung.com/cs/ml-underfitting-overfitting)

### Q 9 : Explain Confusion Matrix, Accuracy, Precision, Recall and F1 Score. ###

Refer to the following links [video tutorial](https://www.youtube.com/watch?v=Kdsp6soqA7o), [blog i](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), [blog ii](https://proclusacademy.com/blog/explainer/confusion-matrix-accuracy-classification-models/) and [blog iii](https://proclusacademy.com/blog/explainer/precision-recall-f1-score-classification-models/)

### Q 10 : Explain the KNN Algorithm. ###

KNN is a simple algorithm, based on the local minimum of the target function which is used to learn an unknown function of desired precision and accuracy. The rest of the answer is [here](https://neptune.ai/blog/knn-algorithm-explanation-opportunities-limitations). And for visual understanding refer this [link](https://www.youtube.com/watch?v=HVXime0nQeI)

### Q 11 : Whats the difference between normalization and regularization ? How regularization affects overfitting ? ###

Normalization is a data preprocessing technique that adjusts the values of features to a common scale, typically between 0 and 1, without distorting the differences in the range of values. This is done to ensure that all features contribute equally to the model, especially when the features have different scales or units. Normalization can help improve the convergence of the learning algorithm and the overall performance of the model.

On the other hand, regularization is a technique used to prevent overfitting in a model by adding a penalty term to the loss function. Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Regularization helps the model generalize better by preventing it from becoming too complex. There are two common types of regularization: L1-norm (Lasso) and L2-norm (Ridge Regression). Both of these methods add a penalty term to the loss function, which encourages the model to use simpler fitting functions and reduces the magnitude of the model parameters.

In summary, normalization is a data preprocessing technique that adjusts the scale of feature values, while regularization is a method used to prevent overfitting by adding a penalty term to the loss function. Both techniques can help improve the performance of a machine learning model, but they serve different purposes and are applied at different stages of the modeling process.

The rest of the answer is [here](https://www.youtube.com/watch?v=NyG-7nRpsW8)

### Q 12 : How k means algorithm performs image compression ? ###

This technique involves clustering the pixels in an image into a smaller number of groups and then representing each group by its mean color. The resulting image will have fewer colors, which reduces the file size, but the overall appearance of the image is still preserved.

Steps for compressing an image using K-means clustering : The rest of the answer is [here](https://www.geeksforgeeks.org/image-compression-using-k-means-clustering/)

### Q 13 : Different techniques for the random initialization of centroids in k means clustering algorithm ###

here are a number of initialization strategies, let's focus on the following : The rest of the answer is [here](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)

### Q 14 : Why are tree-based models robust to outliers? ###

Tree-based models are less affected by outliers because they partition the feature space into regions and make predictions based on the majority class or average target value within each region. Outliers may fall into their own regions, but they have less impact on the overall model prediction compared to linear models, which try to fit a single line or plane to all data points. More more details click [here](https://dimensionless.in/tree-based-models-roboust-outliers/#:~:text=The%20process%20continues%20until%20a,contains%20more%20than%20five%20observations.&text=Since%2C%20extreme%20values%20or%20outliers,methods%20are%20insensitive%20to%20outliers.)

### Q 15 : What is the Huber Loss / Smooth Mean Absolute Error ? When to use Huber Loss ? ###

Huber Loss or Smooth Mean Absolute Error is a loss function that takes the advantageous characteristics of the Mean Absolute Error and Mean Squared Error loss functions and combines them into a single loss function. The hybrid nature of Huber Loss makes it less sensitive to outliers, just like MAE, but also penalizes minor errors within the data sample, similar to MSE. The Huber Loss function is also utilized in regression machine learning tasks. The mathematical equation for Huber Loss is as follows :

`L(δ, y, f(x)) = (1/2) * (f(x) - y)^2   if |f(x) - y| <= δ
               = δ * |f(x) - y| - (1/2) * δ^2   if |f(x) - y| > δ`

Where:

* L represents the Huber Loss function
* δ is the delta parameter, which determines the threshold for switching between the quadratic and linear components of the loss function
* y is the true value or target value
* f(x) is the predicted value

The rest of the answer is [here](https://www.datacamp.com/tutorial/loss-function-in-machine-learning)

### Q 16 : What is interquartile range or IQR ? ###

Interquartile range is the amount of spread in the middle 50 % of a dataset. In other words, it is the distance between the first quartile Q1 and the third quartile Q3. IQR = Q3 - Q1.

### Q 17 : What do you mean by an unbalanced dataset ? ###

An unbalanced dataset in machine learning refers to a dataset where the number of observations in each class is significantly different. This can lead to issues during model training and evaluation, especially for classification problems.

In a binary classification task, for example, if one class has a much larger number of samples than the other, the model may become biased towards the majority class. As a result, the model's performance may be poor on the minority class, leading to lower accuracy, precision, recall, or F1 score for that class.

Dealing with unbalanced datasets often involves techniques like resampling (e.g., oversampling the minority class, undersampling the majority class), using different evaluation metrics (e.g., ROC-AUC, precision-recall curve), or using algorithms that are less sensitive to class imbalance (e.g., ensemble methods, anomaly detection algorithms).

### What are L1 and L2 regularization ? What is the difference between L1 and L2 regularization ? ###

Find the answer [here](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization), and for better understanding, refer to these videos [L2 Regression](https://www.youtube.com/watch?v=Q81RR3yKn30) and [L1 Regression](https://www.youtube.com/watch?v=NGf0voTMlcs)

### Q 18 : What are te assumptions if linear regression ? ###

Mainly there are 7 assumptions taken while using Linear Regression:

* Linear Model
* No Multicolinearlity in the data
* Homoscedasticity of Residuals or Equal Variances
* No Autocorrelation in residuals
* Number of observations Greater than the number of predictors
* Each observation is unique
* Predictors are distributed Normally

Note : For more detail refer to this [article](https://www.geeksforgeeks.org/assumptions-of-linear-regression/) 

### Q 19 : What are support vector machine or SVM ?

A support vector machine (SVM) is a machine learning algorithm that uses supervised learning models to solve complex classification, regression, and outlier detection problems by performing optimal data transformations that determine boundaries between data points based on predefined classes, labels, or outputs. The rest of the answer is [here](https://www.youtube.com/watch?v=efR1C6CvhmE)

### Q 20 : What is cross validation ?

Find the answer [here](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right), and for better understanding, refer to this [video](https://www.youtube.com/watch?v=fSytzGwwBVw)

### Q 21 : What is a dimensionality reduction algorithm ? What is principal component analysis or PCA ? Also what is the curse of dimensionality in detail ?

Dimensionality reduction is the task of reducing the number of features in a dataset. In machine learning tasks there are often too many variables to work with. These variables are also called features. The higher the number of features, the more difficult it is to model them, this is known as the curse of dimensionality.

Additionally, some of these features can be quite redundant, adding noise to the dataset and it makes no sense to have them in the training data. This is where feature space needs to be reduced. 

The process of dimensionality reduction essentially transforms data from high-dimensional feature space to a low-dimensional feature space. Simultaneously, it is also important that meaningful properties present in the data are not lost during the transformation.

Dimensionality reduction is commonly used in data visualization to understand and interpret the data, and in machine learning or deep learning techniques to simplify the task at hand. 

The most popular library for dimensionality reduction is scikit-learn (sklearn). The library consists of three main modules for dimensionality reduction algorithms :

* Decomposition algorithms : Principal Component Analysis, Kernel Principal Component Analysis, Non-Negative Matrix Factorization and Singular Value Decomposition 
* Manifold learning algorithms : t-Distributed Stochastic Neighbor Embedding, Spectral Embedding and Locally Linear Embedding
* Discriminant Analysis : Linear Discriminant Analysis

For more details refer to this [link](https://neptune.ai/blog/dimensionality-reduction)

Principal Component Analysis, or PCA, is a dimensionality-reduction method to find lower-dimensional space by preserving the variance as measured in the high dimensional input space. It is an unsupervised method for dimensionality reduction. 

PCA transformations are linear transformations. It involves the process of finding the principal components, which is the decomposition of the feature matrix into eigenvectors. This means that PCA will not be effective when the distribution of the dataset is non-linear.

Reducing the number of variables of data not only reduces complexity but also decreases the accuracy of the machine learning model. However, with a smaller number of features it is easy to explore, visualize and analyze, it also makes machine learning algorithms computationally less expensive. In simple words, the idea of PCA is to reduce the number of variables of a data set, while preserving as much information as possible.

For more details refer to this [link](https://www.youtube.com/watch?v=FgakZw6K1QQ)

ML/DL algorithms need a large amount of data to learn invariance, patterns and representations. If this data comprises a large number of features, this can lead to the curse of dimensionality. The curse of dimensionality describes that in order to estimate an arbitrary function with a certain accuracy the number of features or dimensionality required for estimation grows exponentially. This is especially true with big data which yields more sparsity. 

Sparsity in data is usually referred to as the features having a value of zero; this doesn’t mean that the value is missing. If the data has a lot of sparse features then the space and computational complexity increase. A model trained on sparse data will perform poorly in the test dataset. In other words, the model during the training learns noise and they are not able to generalize well. Hence they overfit.  

Issues that arise with high dimensional data are:

* Running a risk of overfitting the machine learning model. 
* Difficulty in clustering similar features.
* Increased space and computational time complexity. 

Non-sparse data or dense data on the other hand is data that has non-zero features. Apart from containing non-zero features they also contain information that is both meaningful and non-redundant.

### Q 22 : What are the different methods for splitting a decision tree ? ###

Attribute selection measure is a heuristic for selecting the splitting criterion that partitions data in the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. The rest of the answer is [here](https://www.datacamp.com/tutorial/decision-tree-classification-python)

### 23 : What is non linear PCA or kernel PCA i.e. KPCA ?

The PCA transformations we described previously are linear transformations that are ineffective with the non-linear distribution. To deal with non-linear distribution, the basic idea is to use the kernel trick. The rest of the answer is [here](https://neptune.ai/blog/dimensionality-reduction)

### 24 : What is an autoencoders ? ###

An autoencoder is an artificial neural network used to learn data encodings in an unsupervised manner. In other words, the autoencoder has to look at the data and construct a function that can transform a particular instance of that data into a meaningful representation. Autoencoders have two main components: an encoder and a decoder. The rest of the answer is [here](https://blog.roboflow.com/what-is-an-autoencoder-computer-vision/).

### 25 : Define Leaky ReLU and tanh activation functions. ###

Leaky ReLU was created to solve the dying ReLU problem using the standard ReLU function that makes the neural network die during training. The rest of the answer is [here](https://www.educative.io/answers/what-is-leaky-relu)

The tanh activation function, also called the hyperbolic tangent activation function that transforms input values to produce output values between -1 and 1. The rest of the answer is [here](https://www.educative.io/answers/what-is-the-tanh-activation-function)

### 26 : Define the following convolution layer, dropout layer and pooling layer. ###

Convolutional Layer: A convolutional layer applies a convolution operation to the input, passing the result to the next layer. This operation involves sliding a filter (also called a kernel) over the input and computing the dot product between the filter and the input at each position. This helps the network learn features from the input data.

Dropout Layer: Dropout is a regularization technique used in neural networks to prevent overfitting. In a dropout layer, a random fraction of the input units is set to zero at each update during training. This helps to reduce the reliance of the network on specific input units and forces it to learn more robust features.

Pooling Layer: Pooling layers are used in CNNs to reduce the spatial dimensions (width and height) of the input volume. The most common type of pooling is max pooling, where the maximum value in each subregion of the input is taken to form the output. This helps to reduce the computation and number of parameters in the network while preserving important features.

### Q 27 : Explain vanishing and exploding gradient. ###

During backpropagation, the calculation of (partial) derivatives/gradients in the weight update formula follows the Chain Rule, where gradients in earlier layers are the multiplication of gradients of later layers. The rest of the answer is [here](https://neptune.ai/blog/vanishing-and-exploding-gradients-debugging-monitoring-fixing)

Note : For further understanding refer to this [video](https://www.youtube.com/watch?v=qhXZsFVxGKo)

### 28 : What is gradient clipping for neural network ? ###

Gradient Clipping is a method where the error derivative is changed or clipped to a threshold during backward propagation through the network, and using the clipped gradients to update the weights. The rest of the answer is [here](https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem)

### 29 : What is a Kernels in Machine Learning ? ###

The so-called kernel trick enables us to apply linear models to non-linear data, which is the reason it has gained popularity in science and industry. In addition to classification, which is the task we usually associate them with, kernels can help us solve other problems in the field, such as regression, clustering, and dimensionality reduction. The rest of the answer is [here](https://www.baeldung.com/cs/intuition-behind-kernels-in-machine-learning)

### 30 : What is momentum in deep learning ? ###

Find the answer [here](https://distill.pub/2017/momentum/) and for better understanding refer to this [video](https://www.youtube.com/watch?v=k8fTYJPd3_I)

### 31 : Define the following dropout rate, stride, padding and filter size. ###

Dropout Rate: Dropout is a regularization technique used in neural networks to prevent overfitting. The dropout rate is the proportion of neurons in a layer that are randomly "dropped out," meaning they are ignored during training for that iteration. This helps prevent the network from relying too much on any individual neuron and promotes more robust learning.

Stride: Stride refers to the number of pixels the filter moves across the input image or feature map at each step during convolution. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time. Using larger strides can reduce the size of the output feature map.

Padding: Padding is the process of adding extra border pixels to the input image or feature map. Padding can be "valid" (no padding, resulting in a smaller output size) or "same" (pad to ensure the output size is the same as the input size). Padding is often used to ensure that the spatial dimensions of the output feature map are compatible with the desired operations (e.g., convolution) and to preserve spatial information at the borders of the input.

Filter Size: Also known as kernel size, the filter size refers to the dimensions of the convolutional filter (e.g., 3x3, 5x5) applied to the input image or feature map. The filter moves across the input, and at each position, it computes a dot product between its weights and the corresponding input values to produce a single output value in the output feature map. The filter size determines the receptive field of the convolution operation.

### Q 32 : What are the different types of pooling ? ###

Pooling is performed in neural networks to reduce variance and computation complexity.The three types of pooling operations are :

* Max pooling: The maximum pixel value of the batch is selected.
* Min pooling: The minimum pixel value of the batch is selected.
* Average pooling: The average value of all the pixels in the batch is selected.

The batch here means a group of pixels of size equal to the filter size which is decided based on the size of the image. In the following example, a filter of 9x9 is chosen. The output of the pooling method varies with the varying value of the filter size.

### Q 33 : What is the purpose of optimizer ? What are the different types of optimizers ? ###

An optimizer is an algorithm or function that adapts the neural network’s attributes, like learning rate and weights. Hence, it assists in improving the accuracy and reduces the total loss. You can use various optimizers to modify your learning rate and weights. But selecting the best optimizer in deep learning depends on your application. The rest of the answer is [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6) and for better understanding, refer to this [video](https://www.youtube.com/watch?v=mdKjMPmcWjY)

### Q 34 : What will happen if we initialize all the weights to 0 or 1 ? ###

Initializing all the weights with zeros leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly. Consider a neural network with two hidden units, and assume we initialize all the biases to 0 and the weights with some constant α.

if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs))). If all weights are zeros, which is even worse, every hidden unit will get zero signal. No matter what was the input - if all weights are the same, all units in hidden layer will be the same too.

For better understanding refer to this [page](https://www.deeplearning.ai/ai-notes/initialization/index.html#:~:text=Initializing%20all%20the%20weights%20with,weights%20with%20some%20constant%20%CE%B1.)

### 35 : What is a Quasi-Recurrent Neural Network or QRNN ? ###

A Quasi-Recurrent Neural Network (QRNN) is a type of neural network architecture that combines the strengths of recurrent neural networks (RNNs) and convolutional neural networks (CNNs). QRNNs use convolutional layers to process input sequences in parallel, similar to CNNs, which makes them computationally efficient. At the same time, they maintain a recurrence mechanism, similar to RNNs, allowing them to model sequential dependencies in the data. This combination makes QRNNs well-suited for tasks involving sequential data, such as natural language processing (NLP) tasks like language modeling and text classification.

The sequential nature of RNNs doesn't allow the model to fully utilize the gpu. Since the activations of each timestep are fed into the inputs to find the activations of the next timestep, each timestep must be processed sequentially. A QRNN tries to limit the amount of sequential processing by using 1d convolutional layers to compute gate vectors concurrently and computing the outputs of the model sequentially in pooling layers. The effect of doing most of the computation concurrently in the conv layers and leaving the less computationally intensive tasks to the sequential pooling layers lets you speed up the speed of the network by up to 16 times!

### 36 : What is the significance of dividing by "2m" and squaring the loss in the equation of cost function for linear regression ?  ###

The `1/m` is to "average" the squared error over the number of components so that the number of components doesn't affect the function.So now the question is why there is an extra `1/2`. In short, it doesn't matter. The solution that minimizes `J` as you have written it will also minimize `2J = 1m∑i(h(xi)−yi)2`. The latter function, 2J may seem more "natural," but the factor of 2 does not matter when optimizing. The only reason some authors like to include it is because when you take the derivative with respect to x the 2 goes away.

Squaring the loss has a few implications:

* It penalizes larger errors more than smaller errors, which makes the model more sensitive to outliers.
* It ensures that the cost function is always positive, which simplifies the optimization process.
* It makes the cost function differentiable, which is necessary for gradient-based optimization algorithms like gradient descent to work.

### 37 : Explain the Logistic Regression Cost Function. ###

The graph of the Mean squared error function is non-convex for logistic regression. As we are putting dependent variable x in a non-linear sigmoid function. As gradient descent does not work for non-convex functions, logistic regression model would never be able to converge to optimal values. The rest of the answer is [here](https://www.baeldung.com/cs/cost-function-logistic-regression-logarithmic-expr)

### 38 : Why we don't use Bias in Regularization ? ###

As bias is just the intercepts of segregation. So, there is no point in using them in regularization. Although we can use it, in the case of neural networks it won't make any difference. The rest of the answer is [here](https://medium.com/@shrutijadon/why-we-dont-use-bias-in-regularization-5a86905dfcd6#:~:text=So%20that%20we%20don't,won't%20make%20any%20difference.)

### 39 : What can be done if a straight line in linear regression cannot separate a dataset into positive and negative classes? ###

In linear regression, if a straight line cannot separate the dataset into positive and negative classes, it indicates that the data is not linearly separable. This means that a linear model is not suitable for this dataset, and more complex models, such as nonlinear regression or classification models, may be more appropriate.

We can perform feature engineering i.e.  creating new features that better represent the underlying patterns in the data, which might help a linear model better separate the classes.

### 40 : What is a regularized gradient descent ? ###

Find the answer [here](https://towardsdatascience.com/create-a-gradient-descent-algorithm-with-regularization-from-scratch-in-python-571cb1b46642) and for better understanding refer to this [video]([https://www.youtube.com/watch?v=k8fTYJPd3_I](https://www.youtube.com/watch?v=6v3r9KPM2t0)https://www.youtube.com/watch?v=6v3r9KPM2t0)
