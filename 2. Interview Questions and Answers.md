### Q 1 : What is gradient descent ? ###

Gradient descent is a generic optimization algorithm cable for finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak 
parameters iteratively in order to minimize the cost function. The rest of the answer is [here](https://www.youtube.com/watch?v=IHZwWFHWa-w&t=4s)

### Q 2 : What is feature engineering ? ###

Feature engineering is the process of transforming raw data into features that are suitable for machine learning models. In other words, it is the process of selecting, extracting, 
and transforming the most relevant features from the available data to build more accurate and efficient machine learning models.

The success of machine learning models heavily depends on the quality of the features used to train them. Feature engineering involves a set of techniques that enable us to create 
new features by combining or transforming the existing ones. These techniques help to highlight the most important patterns and relationships in the data, which in turn helps the 
machine learning model to learn from the data more effectively.

### Q 3 : What is feature mapping ? ###

Feature mapping is a technique used in data analysis and machine learning to transform input data from a lower-dimensional space to a higher-dimensional space, where it can be 
more easily analyzed or classified. The rest of the answer is [here](https://www.geeksforgeeks.org/feature-mapping/) 

### Q 4 : What is a neural network ? ###

Neural Network is a sophisticated architecture consist of a stack of layers and neurons in each layer. Neural Network is the mathematical functions which transfer input variables 
to the target variable and learn the patterns.

A Neural Network is basically a dense interconnection of layers, which are further made up of basic units called perceptrons. A perceptron consists of input terminals, the 
processing unit and the output terminals. The input terminals of a perceptron are connected to the output terminals of the preceding perceptrons. The rest of the answer is [here](https://www.youtube.com/watch?v=aircAruvnKk)

### Q 5 : Why do we need to perform normalization or feature scaling ? What are the different types of normalization ? Also what is z-score normalization ? ###

Normalization or feature scaling is a way to make sure that features with very diverse ranges will proportionally impact the network performance. Without normalization, some 
features or variables might be ignored. For example, imagine that we want to predict the price of a car using two features such as the driven distance and the car’s age. 
The first feature’s range is in thousands whereas the second one is in the range of tens of years. Using the raw data for predicting the price of the car, the distance feature 
would outweigh the age feature significantly. Therefore, we should normalize these two features to get a more accurate prediction.

The different types of normalization are as follows : The rest of the answer is [here](https://medium.com/nerd-for-tech/overview-of-normalization-techniques-in-deep-learning-e12a79060daf)

Z-score normalization refers to the process of normalizing every value in a dataset such that the mean of all of the values is 0 and the standard deviation is 1. We use the 
following formula to perform a z-score normalization on every value in a dataset : `New value = (x – μ) / σ` where : `x: Original value, μ: Mean of data and σ: Standard deviation of data`

### Q 6 : What is the difference between normalization or feature scaling and standarization ? ###

Normalisation is suitable to use when the data does not follow Gaussian Distribution principles. It can be used in algorithms that do not assume data distribution, such as 
K-Nearest Neighbors and Neural Networks. The rest of the answer is [here](https://www.codingninjas.com/studio/library/normalisation-vs-standardisation). More visual 
understanding refer to this [link](https://www.youtube.com/watch?v=sxEqtjLC0aM)

### Q 7 : Explain the "Bias-Variance Tradeoff" in Machine Learning. ###

In detail : Imagine a scenario in which a model works perfectly well with the data it was trained on, but provides incorrect predictions when it meets new, unfamiliar data. The rest of the answer is [here](https://serokell.io/blog/bias-variance-tradeoff)

In brief : In order to evaluate the performance of the model, we need to look at the amount of error it’s making. The rest of the answer is [here](https://towardsdatascience.com/bias-variance-trade-off-overfitting-regularization-in-machine-learning-d79c6d8f20b4)

### Q 8 : Explain overfitting and underfitting. ###

Overfitting happens when we train a machine learning model too much tuned to the training set. As a result, the model learns the training data too well, but it can’t generate good predictions for unseen data. An overfitted model produces low accuracy results for data points unseen in training, hence, leads to non-optimal decisions.

Underfitting occurs when the machine learning model is not well-tuned to the training set. The resulting model is not capturing the relationship between input and output well enough. Therefore, it doesn’t produce accurate predictions, even for the training dataset. Resultingly, an underfitted model generates poor results that lead to high-error decisions, like an overfitted model.

![image](https://github.com/roy-sub/Data-Scientist-Interview-Course/blob/main/Figures/overfitting%20vs%20underfitting.png)

Reducing the error from overfitting or underfitting is referred to as the bias-variance tradeoff. We aim to find a good fitting model in between. More further details refer to this [link](https://www.baeldung.com/cs/ml-underfitting-overfitting)

### Q 9 : Explain Confusion Matrix, Accuracy, Precision, Recall and F1 Score. ###

Refer to the following links [video tutorial](https://www.youtube.com/watch?v=Kdsp6soqA7o), [blog i](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), [blog ii](https://proclusacademy.com/blog/explainer/confusion-matrix-accuracy-classification-models/) and [blog iii](https://proclusacademy.com/blog/explainer/precision-recall-f1-score-classification-models/)

### Q 10 : Explain the KNN Algorithm. ###

KNN is a simple algorithm, based on the local minimum of the target function which is used to learn an unknown function of desired precision and accuracy. The rest of the answer is [here](https://neptune.ai/blog/knn-algorithm-explanation-opportunities-limitations). And for visual understanding refer this [link](https://www.youtube.com/watch?v=HVXime0nQeI)

### Q 11 : Whats the difference between normalization and regularization ? How regularization affects overfitting ? ###

Normalization is a data preprocessing technique that adjusts the values of features to a common scale, typically between 0 and 1, without distorting the differences in the range of values. This is done to ensure that all features contribute equally to the model, especially when the features have different scales or units. Normalization can help improve the convergence of the learning algorithm and the overall performance of the model.

On the other hand, regularization is a technique used to prevent overfitting in a model by adding a penalty term to the loss function. Overfitting occurs when a model learns the training data too well, including the noise, and performs poorly on new, unseen data. Regularization helps the model generalize better by preventing it from becoming too complex. There are two common types of regularization: L1-norm (Lasso) and L2-norm (Ridge Regression). Both of these methods add a penalty term to the loss function, which encourages the model to use simpler fitting functions and reduces the magnitude of the model parameters.

In summary, normalization is a data preprocessing technique that adjusts the scale of feature values, while regularization is a method used to prevent overfitting by adding a penalty term to the loss function. Both techniques can help improve the performance of a machine learning model, but they serve different purposes and are applied at different stages of the modeling process.

The rest of the answer is [here](https://www.youtube.com/watch?v=NyG-7nRpsW8)

### Q 12 : How k means algorithm performs image compression ? ###

This technique involves clustering the pixels in an image into a smaller number of groups and then representing each group by its mean color. The resulting image will have fewer colors, which reduces the file size, but the overall appearance of the image is still preserved.

Steps for compressing an image using K-means clustering : The rest of the answer is [here](https://www.geeksforgeeks.org/image-compression-using-k-means-clustering/)

### Q 13 : Different techniques for the random initialization of centroids in k means clustering algorithm ###

here are a number of initialization strategies, let's focus on the following : The rest of the answer is [here](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)

### Q 14 : Why are tree-based models robust to outliers? ###

Tree-based models are less affected by outliers because they partition the feature space into regions and make predictions based on the majority class or average target value within each region. Outliers may fall into their own regions, but they have less impact on the overall model prediction compared to linear models, which try to fit a single line or plane to all data points. More more details click [here](https://dimensionless.in/tree-based-models-roboust-outliers/#:~:text=The%20process%20continues%20until%20a,contains%20more%20than%20five%20observations.&text=Since%2C%20extreme%20values%20or%20outliers,methods%20are%20insensitive%20to%20outliers.)

### Q 15 : What is the Huber Loss / Smooth Mean Absolute Error ? When to use Huber Loss ? ###

Huber Loss or Smooth Mean Absolute Error is a loss function that takes the advantageous characteristics of the Mean Absolute Error and Mean Squared Error loss functions and combines them into a single loss function. The hybrid nature of Huber Loss makes it less sensitive to outliers, just like MAE, but also penalizes minor errors within the data sample, similar to MSE. The Huber Loss function is also utilized in regression machine learning tasks. The mathematical equation for Huber Loss is as follows :

`L(δ, y, f(x)) = (1/2) * (f(x) - y)^2   if |f(x) - y| <= δ
               = δ * |f(x) - y| - (1/2) * δ^2   if |f(x) - y| > δ`

Where:

* L represents the Huber Loss function
* δ is the delta parameter, which determines the threshold for switching between the quadratic and linear components of the loss function
* y is the true value or target value
* f(x) is the predicted value

The rest of the answer is [here](https://www.datacamp.com/tutorial/loss-function-in-machine-learning)

### Q 16 : What is interquartile range or IQR ? ###

Interquartile range is the amount of spread in the middle 50 % of a dataset. In other words, it is the distance between the first quartile Q1 and the third quartile Q3. IQR = Q3 - Q1.

### Q 17 : What do you mean by an unbalanced dataset ? ###

An unbalanced dataset in machine learning refers to a dataset where the number of observations in each class is significantly different. This can lead to issues during model training and evaluation, especially for classification problems.

In a binary classification task, for example, if one class has a much larger number of samples than the other, the model may become biased towards the majority class. As a result, the model's performance may be poor on the minority class, leading to lower accuracy, precision, recall, or F1 score for that class.

Dealing with unbalanced datasets often involves techniques like resampling (e.g., oversampling the minority class, undersampling the majority class), using different evaluation metrics (e.g., ROC-AUC, precision-recall curve), or using algorithms that are less sensitive to class imbalance (e.g., ensemble methods, anomaly detection algorithms).

### What are L1 and L2 regularization ? What is the difference between L1 and L2 regularization ? ###

Find the answer [here](https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization), and for better understanding, refer to these videos [L2 Regression](https://www.youtube.com/watch?v=Q81RR3yKn30) and [L1 Regression](https://www.youtube.com/watch?v=NGf0voTMlcs)

### Q 18 : What are te assumptions if linear regression ? ###

Mainly there are 7 assumptions taken while using Linear Regression:

* Linear Model
* No Multicolinearlity in the data
* Homoscedasticity of Residuals or Equal Variances
* No Autocorrelation in residuals
* Number of observations Greater than the number of predictors
* Each observation is unique
* Predictors are distributed Normally

Note : For more detail refer to this [article](https://www.geeksforgeeks.org/assumptions-of-linear-regression/) 

### Q 19 : What are support vector machine or SVM ?

A support vector machine (SVM) is a machine learning algorithm that uses supervised learning models to solve complex classification, regression, and outlier detection problems by performing optimal data transformations that determine boundaries between data points based on predefined classes, labels, or outputs. The rest of the answer is [here](https://www.youtube.com/watch?v=efR1C6CvhmE)

### Q 20 : What is cross validation ?

Find the answer [here](https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right), and for better understanding, refer to this [video](https://www.youtube.com/watch?v=fSytzGwwBVw)











